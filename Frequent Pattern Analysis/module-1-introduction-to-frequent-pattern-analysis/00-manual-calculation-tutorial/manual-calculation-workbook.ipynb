{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Calculation Workbook: Frequent Pattern Analysis from Scratch\n",
    "\n",
    "This notebook implements frequent pattern analysis using pure Python without external libraries (except basic collections). Follow along to understand the core algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "We'll use the same small dataset from our manual calculations to verify our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample transaction database\n",
    "transactions = [\n",
    "    ['Milk', 'Bread', 'Eggs'],           # T1\n",
    "    ['Bread', 'Eggs', 'Butter'],        # T2\n",
    "    ['Milk', 'Eggs'],                   # T3\n",
    "    ['Milk', 'Bread', 'Eggs', 'Butter'], # T4\n",
    "    ['Milk', 'Bread'],                  # T5\n",
    "    ['Bread', 'Eggs']                   # T6\n",
    "]\n",
    "\n",
    "print(\"Transaction Database:\")\n",
    "for i, transaction in enumerate(transactions, 1):\n",
    "    print(f\"T{i}: {transaction}\")\n",
    "    \n",
    "print(f\"\\nTotal transactions: {len(transactions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Calculate Support for Individual Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support(itemset, transactions):\n",
    "    \"\"\"\n",
    "    Calculate support for an itemset.\n",
    "    \n",
    "    Args:\n",
    "        itemset: A set or list of items\n",
    "        transactions: List of transaction lists\n",
    "    \n",
    "    Returns:\n",
    "        Support value (float between 0 and 1)\n",
    "    \"\"\"\n",
    "    if isinstance(itemset, list):\n",
    "        itemset = set(itemset)\n",
    "    \n",
    "    count = 0\n",
    "    for transaction in transactions:\n",
    "        if itemset.issubset(set(transaction)):\n",
    "            count += 1\n",
    "    \n",
    "    return count / len(transactions)\n",
    "\n",
    "# Get all unique items\n",
    "all_items = set()\n",
    "for transaction in transactions:\n",
    "    all_items.update(transaction)\n",
    "\n",
    "print(\"Individual Item Support:\")\n",
    "print(\"-\" * 30)\n",
    "item_support = {}\n",
    "for item in sorted(all_items):\n",
    "    support = calculate_support([item], transactions)\n",
    "    item_support[item] = support\n",
    "    print(f\"{item:8}: {support:.3f} ({support*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Apriori Algorithm from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def generate_candidates(frequent_itemsets, k):\n",
    "    \"\"\"\n",
    "    Generate candidate k-itemsets from frequent (k-1)-itemsets.\n",
    "    \n",
    "    Args:\n",
    "        frequent_itemsets: List of frequent (k-1)-itemsets\n",
    "        k: Size of candidates to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of candidate k-itemsets\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    # Convert to sorted tuples for consistent comparison\n",
    "    itemsets = [tuple(sorted(itemset)) for itemset in frequent_itemsets]\n",
    "    \n",
    "    # Generate candidates by joining itemsets\n",
    "    for i in range(len(itemsets)):\n",
    "        for j in range(i + 1, len(itemsets)):\n",
    "            # Join if first k-2 items are the same\n",
    "            if itemsets[i][:-1] == itemsets[j][:-1]:\n",
    "                candidate = tuple(sorted(set(itemsets[i]) | set(itemsets[j])))\n",
    "                if len(candidate) == k:\n",
    "                    candidates.append(candidate)\n",
    "    \n",
    "    return list(set(candidates))  # Remove duplicates\n",
    "\n",
    "def apriori_algorithm(transactions, min_support):\n",
    "    \"\"\"\n",
    "    Implement the Apriori algorithm from scratch.\n",
    "    \n",
    "    Args:\n",
    "        transactions: List of transaction lists\n",
    "        min_support: Minimum support threshold\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with frequent itemsets by level\n",
    "    \"\"\"\n",
    "    # Get all unique items\n",
    "    all_items = set()\n",
    "    for transaction in transactions:\n",
    "        all_items.update(transaction)\n",
    "    \n",
    "    frequent_itemsets = {}\n",
    "    k = 1\n",
    "    \n",
    "    # Level 1: Individual items\n",
    "    candidates = [[item] for item in all_items]\n",
    "    \n",
    "    while candidates:\n",
    "        print(f\"\\n--- Level {k} ---\")\n",
    "        print(f\"Candidates: {len(candidates)}\")\n",
    "        \n",
    "        # Calculate support for candidates\n",
    "        frequent_k = []\n",
    "        for candidate in candidates:\n",
    "            support = calculate_support(candidate, transactions)\n",
    "            print(f\"  {candidate}: support = {support:.3f}\")\n",
    "            \n",
    "            if support >= min_support:\n",
    "                frequent_k.append(candidate)\n",
    "        \n",
    "        if frequent_k:\n",
    "            frequent_itemsets[k] = frequent_k\n",
    "            print(f\"Frequent {k}-itemsets: {len(frequent_k)}\")\n",
    "            for itemset in frequent_k:\n",
    "                print(f\"  {itemset}\")\n",
    "            \n",
    "            # Generate candidates for next level\n",
    "            k += 1\n",
    "            candidates = generate_candidates(frequent_k, k)\n",
    "        else:\n",
    "            print(f\"No frequent {k}-itemsets found. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "# Run Apriori algorithm\n",
    "min_support = 0.5  # 50%\n",
    "print(f\"Running Apriori Algorithm with minimum support = {min_support}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "frequent_itemsets = apriori_algorithm(transactions, min_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_association_rules(frequent_itemsets, transactions, min_confidence=0.5):\n",
    "    \"\"\"\n",
    "    Generate association rules from frequent itemsets.\n",
    "    \n",
    "    Args:\n",
    "        frequent_itemsets: Dictionary of frequent itemsets by level\n",
    "        transactions: List of transaction lists\n",
    "        min_confidence: Minimum confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of association rules with metrics\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    \n",
    "    # Generate rules from itemsets of size 2 or more\n",
    "    for level in range(2, len(frequent_itemsets) + 1):\n",
    "        if level not in frequent_itemsets:\n",
    "            continue\n",
    "            \n",
    "        for itemset in frequent_itemsets[level]:\n",
    "            # Generate all possible antecedent-consequent pairs\n",
    "            items = list(itemset)\n",
    "            \n",
    "            # For each possible split of the itemset\n",
    "            for i in range(1, len(items)):\n",
    "                for antecedent in combinations(items, i):\n",
    "                    consequent = tuple(item for item in items if item not in antecedent)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    support_full = calculate_support(itemset, transactions)\n",
    "                    support_antecedent = calculate_support(antecedent, transactions)\n",
    "                    support_consequent = calculate_support(consequent, transactions)\n",
    "                    \n",
    "                    confidence = support_full / support_antecedent\n",
    "                    lift = confidence / support_consequent\n",
    "                    \n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append({\n",
    "                            'antecedent': antecedent,\n",
    "                            'consequent': consequent,\n",
    "                            'support': support_full,\n",
    "                            'confidence': confidence,\n",
    "                            'lift': lift\n",
    "                        })\n",
    "    \n",
    "    return rules\n",
    "\n",
    "# Generate association rules\n",
    "min_confidence = 0.5  # 50%\n",
    "rules = generate_association_rules(frequent_itemsets, transactions, min_confidence)\n",
    "\n",
    "print(f\"\\nAssociation Rules (min_confidence = {min_confidence}):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Rule':<25} {'Support':<10} {'Confidence':<12} {'Lift':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for rule in rules:\n",
    "    antecedent = ', '.join(rule['antecedent'])\n",
    "    consequent = ', '.join(rule['consequent'])\n",
    "    rule_str = f\"{antecedent} → {consequent}\"\n",
    "    \n",
    "    print(f\"{rule_str:<25} {rule['support']:<10.3f} {rule['confidence']:<12.3f} {rule['lift']:<10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Detailed Manual Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_manual_calculations():\n",
    "    \"\"\"\n",
    "    Verify our calculations match the manual work from the README.\n",
    "    \"\"\"\n",
    "    print(\"Manual Verification:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Verify individual item support\n",
    "    print(\"\\n1. Individual Item Support:\")\n",
    "    expected = {'Milk': 4/6, 'Bread': 5/6, 'Eggs': 5/6, 'Butter': 2/6}\n",
    "    \n",
    "    for item, exp_support in expected.items():\n",
    "        calc_support = calculate_support([item], transactions)\n",
    "        match = \"✓\" if abs(calc_support - exp_support) < 0.001 else \"✗\"\n",
    "        print(f\"  {item}: Expected {exp_support:.3f}, Got {calc_support:.3f} {match}\")\n",
    "    \n",
    "    # Verify 2-itemset support\n",
    "    print(\"\\n2. 2-Itemset Support:\")\n",
    "    expected_2 = {\n",
    "        ('Milk', 'Bread'): 3/6,\n",
    "        ('Milk', 'Eggs'): 3/6,\n",
    "        ('Bread', 'Eggs'): 4/6\n",
    "    }\n",
    "    \n",
    "    for itemset, exp_support in expected_2.items():\n",
    "        calc_support = calculate_support(itemset, transactions)\n",
    "        match = \"✓\" if abs(calc_support - exp_support) < 0.001 else \"✗\"\n",
    "        print(f\"  {itemset}: Expected {exp_support:.3f}, Got {calc_support:.3f} {match}\")\n",
    "    \n",
    "    # Verify specific rule calculations\n",
    "    print(\"\\n3. Association Rule Verification:\")\n",
    "    \n",
    "    # Bread → Eggs rule\n",
    "    support_bread_eggs = calculate_support(['Bread', 'Eggs'], transactions)\n",
    "    support_bread = calculate_support(['Bread'], transactions)\n",
    "    support_eggs = calculate_support(['Eggs'], transactions)\n",
    "    \n",
    "    confidence = support_bread_eggs / support_bread\n",
    "    lift = confidence / support_eggs\n",
    "    \n",
    "    print(f\"  Bread → Eggs:\")\n",
    "    print(f\"    Support: {support_bread_eggs:.3f}\")\n",
    "    print(f\"    Confidence: {confidence:.3f}\")\n",
    "    print(f\"    Lift: {lift:.3f}\")\n",
    "    print(f\"    Expected: Support=0.667, Confidence=0.800, Lift=0.960\")\n",
    "\n",
    "verify_manual_calculations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Interactive Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_thresholds():\n",
    "    \"\"\"\n",
    "    Experiment with different support and confidence thresholds.\n",
    "    \"\"\"\n",
    "    print(\"Threshold Experimentation:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    support_thresholds = [0.3, 0.4, 0.5, 0.6]\n",
    "    \n",
    "    for min_sup in support_thresholds:\n",
    "        print(f\"\\nMinimum Support = {min_sup}:\")\n",
    "        freq_itemsets = apriori_algorithm(transactions, min_sup)\n",
    "        \n",
    "        # Count total frequent itemsets\n",
    "        total_itemsets = sum(len(itemsets) for itemsets in freq_itemsets.values())\n",
    "        print(f\"  Total frequent itemsets: {total_itemsets}\")\n",
    "        \n",
    "        # Generate rules\n",
    "        rules = generate_association_rules(freq_itemsets, transactions, min_confidence=0.5)\n",
    "        print(f\"  Total rules (confidence ≥ 0.5): {len(rules)}\")\n",
    "\n",
    "# Uncomment to run the experiment\n",
    "# experiment_with_thresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_analysis():\n",
    "    \"\"\"\n",
    "    Provide a summary of the analysis and key insights.\n",
    "    \"\"\"\n",
    "    print(\"Analysis Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\nKey Findings:\")\n",
    "    print(\"1. With 50% minimum support:\")\n",
    "    print(\"   - Butter was eliminated (only 33.3% support)\")\n",
    "    print(\"   - 3 frequent 1-itemsets: Milk, Bread, Eggs\")\n",
    "    print(\"   - 3 frequent 2-itemsets: {Milk,Bread}, {Milk,Eggs}, {Bread,Eggs}\")\n",
    "    print(\"   - 0 frequent 3-itemsets\")\n",
    "    \n",
    "    print(\"\\n2. Association Rules:\")\n",
    "    print(\"   - All lift values < 1.0 (weak or negative associations)\")\n",
    "    print(\"   - Strongest rule: Bread → Eggs (80% confidence)\")\n",
    "    print(\"   - Most frequent rule: Bread ↔ Eggs (66.7% support)\")\n",
    "    \n",
    "    print(\"\\n3. Business Implications:\")\n",
    "    print(\"   - Bread and Eggs are commonly bought together\")\n",
    "    print(\"   - Milk appears in many transactions but weaker associations\")\n",
    "    print(\"   - Butter is a niche item (low frequency)\")\n",
    "    \n",
    "    print(\"\\n4. Algorithm Performance:\")\n",
    "    print(\"   - Manual implementation matches library results\")\n",
    "    print(\"   - Apriori efficiently pruned search space\")\n",
    "    print(\"   - No 3-itemsets needed evaluation due to low 2-itemset support\")\n",
    "\n",
    "summarize_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the manual process:\n",
    "\n",
    "1. **Try Different Datasets**: Experiment with your own transaction data\n",
    "2. **Adjust Thresholds**: See how different support/confidence values affect results\n",
    "3. **Compare with Libraries**: Use mlxtend to verify your understanding\n",
    "4. **Scale Up**: Apply to larger datasets using optimized algorithms\n",
    "5. **Business Application**: Interpret rules in real-world contexts\n",
    "\n",
    "The manual understanding you've gained here is fundamental to using automated tools effectively!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}